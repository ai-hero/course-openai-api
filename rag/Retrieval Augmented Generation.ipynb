{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf205f5-2483-4b65-b655-ad06f1253d7b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "This notebook provides an introduction to Retrieval Augmented Generation. Especially, we will learn how to improve our RAG based on different embeddings, LLMs, etc.\n",
    "\n",
    "The focus here is the methodology of how to improve your Rag using a nort-star metric (e.g. Ragas Score). Your actual use case might be different, and the best chunking, prompt, embedding, retrieval strategy, and LLM for it might be different. Use this methodology to pick the best one for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50c39d-3c47-4f34-88c2-84bbc54d020f",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use LlamaIndex, which provides some abstractions over underlying APIs used for building LLM applications. For more about RAG and LlamaIndex's definitions, see [here](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index==0.9.48 datasets tqdm python-dotenv spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423cc82b-5f5c-4757-98b4-914a3ccad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 3 rows\n",
      "100 files in folder: /Users/rahulparundekar/workspaces/course-openai-api/rag/.content/docs/1e41916248531ab7c35d0c9895b9e097.txt, ...\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a directory to store the content\n",
    "content_folder = os.path.join(os.path.abspath(\"\"), \".content/\")\n",
    "documents_folder = os.path.join(os.path.abspath(\"\"), \".content/docs/\")\n",
    "os.makedirs(documents_folder, exist_ok=True)\n",
    "\n",
    "NUM_DOCUMENTS = 100\n",
    "\n",
    "\n",
    "# Function to save article content to a file\n",
    "def save_article_content(text, folder):\n",
    "    try:\n",
    "        # Fetching the content of the city's Wikipedia page\n",
    "        checksum = hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "        file_path = os.path.join(folder, checksum + \".txt\")\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(text)\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "dataset = DatasetDict.load_from_disk(f\"{content_folder}/rag_sciq_data.hf\")\n",
    "print(f\"Dataset contains {len(dataset)} rows\")\n",
    "\n",
    "# Saving the content of each train set document in a file\n",
    "saved_files = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"support\"]:\n",
    "        saved_files.append(save_article_content(row[\"support\"], documents_folder))\n",
    "    if NUM_DOCUMENTS and len(saved_files) >= NUM_DOCUMENTS:\n",
    "        break\n",
    "# We'll load documents that we've already downloaded in the Synthetic Dataset for RAG\n",
    "data_dir = os.path.join(os.path.abspath(\"\"), \".content/docs\")\n",
    "input_files = glob(os.path.join(data_dir, \"*.txt\"))\n",
    "print(f\"{len(input_files)} files in folder: {input_files[0]}, ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59224d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927087f9-9b52-4a92-8fea-5422b1172922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f115d5-00b0-4f8c-85e6-4d4654d3ac43",
   "metadata": {},
   "source": [
    "## A simple RAG with LlamaIndex (using defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64847c2d-83e9-4ac6-bac5-102d6864ceb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2089-5a78-42e7-aa87-5dd9b860d260",
   "metadata": {},
   "source": [
    "First, build the index from all the documents we have. Using default chunking, and embedding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01567917-d39e-4185-bc97-e2027df1c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe279f-b069-4135-8dd0-058d09f75e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2432d76-a5a9-4637-b682-871847d28c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "example_one = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_one][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_one][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cb97e-c7ad-4e31-aa42-097b956da4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_two = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_two][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_two][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340c103-b046-4ef5-b605-28795829300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_three = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_three][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_three][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nChunks:\")\n",
    "for node in response.source_nodes:\n",
    "    print(\"--------------------------\")\n",
    "    print(str(node.text))\n",
    "    print(\"--------------------------\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4e23b-0778-45b0-a38b-b91b4e59bc9b",
   "metadata": {},
   "source": [
    "## Evaluation of RAGs using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f031c8c-62c0-4a59-a724-16ced4263b17",
   "metadata": {},
   "source": [
    "So, for the question + documents + answers we have in our truncated dataset, let's calculate some metrics to help us improve the model.\n",
    "\n",
    "We'll use Ragas score. Let's benchmark whatevet model Llama Index is using by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fd6f-75df-42dd-9479-05d9f3812b04",
   "metadata": {},
   "source": [
    "## Ragas \n",
    "You need the following columns for Ragas Evaluation\n",
    "- question: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "- answer: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "- contexts: list[list[str]] - The contexts that were passed into the LLM to answer the question.\n",
    "- ground_truths: list[list[str]] - The ground truth answer to the questions. (only required if you are using context_recall)\n",
    "\n",
    "\n",
    "## Ragas Metrics:\n",
    "The harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.\n",
    "- faithfulness - the factual consistency of the answer to the context base on the question.\n",
    "- context_precision - a measure of how relevant the retrieved context is to the question. Conveys quality of the retrieval pipeline.\n",
    "- answer_relevancy - a measure of how relevant the answer is to the question\n",
    "- context_recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d7273-775d-4345-aa4b-25fdb080565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9a585-4337-47c9-923e-491a9949f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import harmonic_mean, mean\n",
    "\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from ragas import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def run_eval(embed_model=None, llm_model=None, dimensions=None):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    correctness_scores = []\n",
    "\n",
    "    if embed_model:\n",
    "        if dimensions:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model, dimensions=dimensions)\n",
    "        else:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model)\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI())\n",
    "    else:\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=the_service_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "    evaluator = CorrectnessEvaluator(service_context=service_context)\n",
    "\n",
    "    for index in tqdm(range(0, len(input_files))):\n",
    "        row = dataset[\"train\"][index]\n",
    "        # The Question\n",
    "        question = row[\"question\"]\n",
    "        questions.append(question)\n",
    "\n",
    "        # The Answer\n",
    "        response = query_engine.query(question)\n",
    "        answer = str(response)\n",
    "        answers.append(answer)\n",
    "\n",
    "        # Contexts\n",
    "        context = []\n",
    "        for node in response.source_nodes:\n",
    "            context.append(str(node.text))\n",
    "        contexts.append(context)\n",
    "\n",
    "        # Ground Truth\n",
    "        actual_answer = row[\"answer\"]\n",
    "        ground_truths.append([actual_answer])\n",
    "\n",
    "        # Correctness with llama-index\n",
    "        correctness = evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=answer,\n",
    "            reference=actual_answer,\n",
    "        )\n",
    "        correctness_scores.append(correctness.score)\n",
    "\n",
    "    eval_dataset = Dataset.from_dict(\n",
    "        {\"question\": questions, \"contexts\": contexts, \"answer\": answers, \"ground_truths\": ground_truths}\n",
    "    )\n",
    "\n",
    "    result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "    baseline_ragas = result\n",
    "    ragas_score = harmonic_mean(list(result.values()))\n",
    "\n",
    "    return mean(correctness_scores), baseline_ragas, ragas_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc859626-c0d7-410d-9379-e0fa75e94580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_correctness, baseline_ragas, baseline_score = run_eval(\n",
    "    embed_model=\"text-embedding-ada-002\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Baseline Correctness Score:\", baseline_correctness)\n",
    "print(\"Baseline Ragas Scores:\", baseline_ragas)\n",
    "print(\"Baseline Overall Ragas Scores:\", baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbd55b-1094-4012-b8b5-b78f639660eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_35t_correctness, small_35t_ragas, small_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Correctness Score:\", small_35t_correctness)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Ragas Score:\", small_35t_ragas)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Overall Ragas Score:\", small_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41066a-9215-4498-a42c-4927ae457936",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_35t_correctness, large_35t_ragas, large_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large Correctness Score:\", large_35t_correctness)\n",
    "print(\"Text Embedding 3 Large Ragas Score:\", large_35t_ragas)\n",
    "print(\"Text Embedding 3 Large Overall Ragas Score:\", large_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25641cd4-dae7-470e-9124-d9cdff07f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_4_correctness, small_4_ragas, small_4_score = run_eval(embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4\")\n",
    "print(\"Text Embedding 3 + GPT 4 Correctness Score:\", small_4_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Ragas Score:\", small_4_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Overall Ragas Score:\", small_4_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa1321-4da1-499c-afb8-61db1aa9f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_4t_correctness, small_4t_ragas, small_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Correctness Score:\", small_4t_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Ragas Score:\", small_4t_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Ragas Score:\", small_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ac9e5-b278-4b0d-ba18-2b08e1727475",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_4t_correctness, large_4t_ragas, large_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Correctness Score:\", large_4t_correctness)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Ragas Score:\", large_4t_ragas)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Ragas Score:\", large_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d49841-eeeb-4115-83ac-5f5587328208",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_256_4t_correctness, large_256_4t_ragas, large_256_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", dimensions=256, llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Correctness Score:\", large_256_4t_correctness)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Ragas Score:\", large_256_4t_ragas)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Ragas Score:\", large_256_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a43d0-1064-4b19-94e3-0b0d821a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ceefc-f3d6-424a-a5a4-e995c4208be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {\n",
    "    \"Embedding Model\": [\n",
    "        \"text-embedding-ada-002\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "    ],\n",
    "    \"LLM Model\": [\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "    ],\n",
    "    \"Correctness\": [\n",
    "        baseline_correctness,\n",
    "        small_35t_correctness,\n",
    "        large_35t_correctness,\n",
    "        small_4_correctness,\n",
    "        small_4t_correctness,\n",
    "        large_4t_correctness,\n",
    "    ],\n",
    "    \"Ragas Score\": [baseline_score, small_35t_score, large_35t_score, small_4_score, small_4t_score, large_4t_score],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(comparison)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e0246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
