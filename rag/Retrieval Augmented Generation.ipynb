{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf205f5-2483-4b65-b655-ad06f1253d7b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "This notebook provides an introduction to Retrieval Augmented Generation. Especially, we will learn how to improve our RAG based on different embeddings, LLMs, etc.\n",
    "\n",
    "The focus here is the methodology of how to improve your Rag using a nort-star metric (e.g. Ragas Score). Your actual use case might be different, and the best chunking, prompt, embedding, retrieval strategy, and LLM for it might be different. Use this methodology to pick the best one for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50c39d-3c47-4f34-88c2-84bbc54d020f",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use LlamaIndex, which provides some abstractions over underlying APIs used for building LLM applications. For more about RAG and LlamaIndex's definitions, see [here](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "423cc82b-5f5c-4757-98b4-914a3ccad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 3 rows\n",
      "100 files in folder: /Users/rahulparundekar/workspaces/course-openai-api/rag/.content/docs/1e41916248531ab7c35d0c9895b9e097.txt, ...\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a directory to store the content\n",
    "content_folder = os.path.join(os.path.abspath(\"\"), \".content/\")\n",
    "documents_folder = os.path.join(os.path.abspath(\"\"), \".content/docs/\")\n",
    "os.makedirs(documents_folder, exist_ok=True)\n",
    "\n",
    "NUM_DOCUMENTS = 100\n",
    "\n",
    "\n",
    "# Function to save article content to a file\n",
    "def save_article_content(text, folder):\n",
    "    try:\n",
    "        # Fetching the content of the city's Wikipedia page\n",
    "        checksum = hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "        file_path = os.path.join(folder, checksum + \".txt\")\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(text)\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "dataset = DatasetDict.load_from_disk(f\"{content_folder}/rag_sciq_data.hf\")\n",
    "print(f\"Dataset contains {len(dataset)} rows\")\n",
    "\n",
    "# Saving the content of each train set document in a file\n",
    "saved_files = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"support\"]:\n",
    "        saved_files.append(save_article_content(row[\"support\"], documents_folder))\n",
    "    if NUM_DOCUMENTS and len(saved_files) >= NUM_DOCUMENTS:\n",
    "        break\n",
    "# We'll load documents that we've already downloaded in the Synthetic Dataset for RAG\n",
    "data_dir = os.path.join(os.path.abspath(\"\"), \".content/docs\")\n",
    "input_files = glob(os.path.join(data_dir, \"*.txt\"))\n",
    "print(f\"{len(input_files)} files in folder: {input_files[0]}, ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "927087f9-9b52-4a92-8fea-5422b1172922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f115d5-00b0-4f8c-85e6-4d4654d3ac43",
   "metadata": {},
   "source": [
    "## A simple RAG with LlamaIndex (using defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64847c2d-83e9-4ac6-bac5-102d6864ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2089-5a78-42e7-aa87-5dd9b860d260",
   "metadata": {},
   "source": [
    "First, build the index from all the documents we have. Using default chunking, and embedding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01567917-d39e-4185-bc97-e2027df1c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 3599.70file/s]\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4afe279f-b069-4135-8dd0-058d09f75e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2432d76-a5a9-4637-b682-871847d28c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What occurs when the immune system attacks a harmless substance that enters the body from the outside?\n",
      "\n",
      "Answer:\n",
      "When the immune system attacks a harmless substance that enters the body from the outside, an allergy occurs.\n",
      "\n",
      "Expected Answer:\n",
      "Allergy\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "example_one = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_one][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_one][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "710cb97e-c7ad-4e31-aa42-097b956da4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "The diatoms are unicellular photosynthetic protists that encase themselves in intricately patterned, glassy cell walls composed of silicon dioxide in a matrix of organic particles. these protists are a component of freshwater and these?\n",
      "\n",
      "Answer:\n",
      "Diatoms are a component of freshwater and marine plankton.\n",
      "\n",
      "Expected Answer:\n",
      "Marine Plankton\n"
     ]
    }
   ],
   "source": [
    "example_two = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_two][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_two][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0340c103-b046-4ef5-b605-28795829300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Transform faults are the site of massive what?\n",
      "\n",
      "Chunks:\n",
      "--------------------------\n",
      "Transform faults are the site of massive earthquakes.\n",
      "--------------------------\n",
      "--------------------------\n",
      "Divergent plate boundaries produce huge mountain ranges under water in every ocean basin.\n",
      "--------------------------\n",
      "\n",
      "Answer:\n",
      "earthquakes\n",
      "\n",
      "Expected Answer:\n",
      "Earthquakes\n"
     ]
    }
   ],
   "source": [
    "example_three = randint(0, len(input_files))\n",
    "question = dataset[\"train\"][example_three][\"question\"]\n",
    "expected_answer = dataset[\"train\"][example_three][\"answer\"]\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nChunks:\")\n",
    "for node in response.source_nodes:\n",
    "    print(\"--------------------------\")\n",
    "    print(str(node.text))\n",
    "    print(\"--------------------------\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(str(response))\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4e23b-0778-45b0-a38b-b91b4e59bc9b",
   "metadata": {},
   "source": [
    "## Evaluation of RAGs using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f031c8c-62c0-4a59-a724-16ced4263b17",
   "metadata": {},
   "source": [
    "So, for the question + documents + answers we have in our truncated dataset, let's calculate some metrics to help us improve the model.\n",
    "\n",
    "We'll use Ragas score. Let's benchmark whatevet model Llama Index is using by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fd6f-75df-42dd-9479-05d9f3812b04",
   "metadata": {},
   "source": [
    "## Ragas \n",
    "You need the following columns for Ragas Evaluation\n",
    "- question: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "- answer: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "- contexts: list[list[str]] - The contexts that were passed into the LLM to answer the question.\n",
    "- ground_truths: list[list[str]] - The ground truth answer to the questions. (only required if you are using context_recall)\n",
    "\n",
    "\n",
    "## Ragas Metrics:\n",
    "The harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.\n",
    "- faithfulness - the factual consistency of the answer to the context base on the question.\n",
    "- context_precision - a measure of how relevant the retrieved context is to the question. Conveys quality of the retrieval pipeline.\n",
    "- answer_relevancy - a measure of how relevant the answer is to the question\n",
    "- context_recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc4d7273-775d-4345-aa4b-25fdb080565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91c9a585-4337-47c9-923e-491a9949f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import harmonic_mean, mean\n",
    "\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from ragas import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def run_eval(embed_model=None, llm_model=None, dimensions=None):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    correctness_scores = []\n",
    "\n",
    "    if embed_model:\n",
    "        if dimensions:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model, dimensions=dimensions)\n",
    "        else:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model)\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI())\n",
    "    else:\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=the_service_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "    evaluator = CorrectnessEvaluator(service_context=service_context)\n",
    "\n",
    "    for index in tqdm(range(0, len(input_files))):\n",
    "        row = dataset[\"train\"][index]\n",
    "        # The Question\n",
    "        question = row[\"question\"]\n",
    "        questions.append(question)\n",
    "\n",
    "        # The Answer\n",
    "        response = query_engine.query(question)\n",
    "        answer = str(response)\n",
    "        answers.append(answer)\n",
    "\n",
    "        # Contexts\n",
    "        context = []\n",
    "        for node in response.source_nodes:\n",
    "            context.append(str(node.text))\n",
    "        contexts.append(context)\n",
    "\n",
    "        # Ground Truth\n",
    "        actual_answer = row[\"answer\"]\n",
    "        ground_truths.append([actual_answer])\n",
    "\n",
    "        # Correctness with llama-index\n",
    "        correctness = evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=answer,\n",
    "            reference=actual_answer,\n",
    "        )\n",
    "        correctness_scores.append(correctness.score)\n",
    "\n",
    "    eval_dataset = Dataset.from_dict(\n",
    "        {\"question\": questions, \"contexts\": contexts, \"answer\": answers, \"ground_truths\": ground_truths}\n",
    "    )\n",
    "\n",
    "    result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "    baseline_ragas = result\n",
    "    ragas_score = harmonic_mean(list(result.values()))\n",
    "\n",
    "    return mean(correctness_scores), baseline_ragas, ragas_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc859626-c0d7-410d-9379-e0fa75e94580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 3893.17file/s]\n",
      "100%|██████████| 100/100 [02:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:28<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:37<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:15<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Correctness Score: 4.41\n",
      "Baseline Ragas Scores: {'context_precision': 0.7900, 'faithfulness': 0.9108, 'answer_relevancy': 0.9102, 'context_recall': 0.9250}\n",
      "Baseline Overall Ragas Scores: 0.880392854087908\n"
     ]
    }
   ],
   "source": [
    "baseline_correctness, baseline_ragas, baseline_score = run_eval(\n",
    "    embed_model=\"text-embedding-ada-002\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Baseline Correctness Score:\", baseline_correctness)\n",
    "print(\"Baseline Ragas Scores:\", baseline_ragas)\n",
    "print(\"Baseline Overall Ragas Scores:\", baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8bbbd55b-1094-4012-b8b5-b78f639660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 4224.72file/s]\n",
      "100%|██████████| 100/100 [02:48<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:25<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:41<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 small + GPT 3.5 Turbo Small Correctness Score: 4.435\n",
      "Text Embedding 3 small + GPT 3.5 Turbo Small Ragas Score: {'context_precision': 0.7800, 'faithfulness': 0.8900, 'answer_relevancy': 0.9107, 'context_recall': 0.9511}\n",
      "Text Embedding 3 small + GPT 3.5 Turbo Overall Ragas Score: 0.8781403113208264\n"
     ]
    }
   ],
   "source": [
    "small_35t_correctness, small_35t_ragas, small_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Correctness Score:\", small_35t_correctness)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Ragas Score:\", small_35t_ragas)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Overall Ragas Score:\", small_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba41066a-9215-4498-a42c-4927ae457936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 1160.89file/s]\n",
      "100%|██████████| 100/100 [02:55<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:09<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:31<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:34<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 Large Correctness Score: 4.425\n",
      "Text Embedding 3 Large Ragas Score: {'context_precision': 0.8000, 'faithfulness': 0.9017, 'answer_relevancy': 0.9107, 'context_recall': 0.9173}\n",
      "Text Embedding 3 Large Overall Ragas Score: 0.8796444887395171\n"
     ]
    }
   ],
   "source": [
    "large_35t_correctness, large_35t_ragas, large_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large Correctness Score:\", large_35t_correctness)\n",
    "print(\"Text Embedding 3 Large Ragas Score:\", large_35t_ragas)\n",
    "print(\"Text Embedding 3 Large Overall Ragas Score:\", large_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "25641cd4-dae7-470e-9124-d9cdff07f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 1373.33file/s]\n",
      "100%|██████████| 100/100 [05:41<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:13<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:30<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:36<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 + GPT 4 Correctness Score: 4.275\n",
      "Text Embedding 3 + GPT 4 Ragas Score: {'context_precision': 0.8000, 'faithfulness': 0.9030, 'answer_relevancy': 0.8809, 'context_recall': 0.9463}\n",
      "Text Embedding 3 + GPT 4 Overall Ragas Score: 0.8792278757290714\n"
     ]
    }
   ],
   "source": [
    "small_4_correctness, small_4_ragas, small_4_score = run_eval(embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4\")\n",
    "print(\"Text Embedding 3 + GPT 4 Correctness Score:\", small_4_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Ragas Score:\", small_4_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Overall Ragas Score:\", small_4_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "04fa1321-4da1-499c-afb8-61db1aa9f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 1768.41file/s]\n",
      "100%|██████████| 100/100 [04:14<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:09<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:30<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:34<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 + GPT 4 Turbo Small Correctness Score: 4.405\n",
      "Text Embedding 3 + GPT 4 Turbo Small Ragas Score: {'context_precision': 0.8000, 'faithfulness': 0.8983, 'answer_relevancy': 0.9099, 'context_recall': 0.9486}\n",
      "Text Embedding 3 + GPT 4 Turbo Ragas Score: 0.885669350582506\n"
     ]
    }
   ],
   "source": [
    "small_4t_correctness, small_4t_ragas, small_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Correctness Score:\", small_4t_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Ragas Score:\", small_4t_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Ragas Score:\", small_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "671ac9e5-b278-4b0d-ba18-2b08e1727475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 1519.85file/s]\n",
      "100%|██████████| 100/100 [04:26<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:30<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:02<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:16<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 Large + GPT 4 Turbo Small Correctness Score: 4.445\n",
      "Text Embedding 3 Large + GPT 4 Turbo Small Ragas Score: {'context_precision': 0.8000, 'faithfulness': 0.9063, 'answer_relevancy': 0.9217, 'context_recall': 0.9116}\n",
      "Text Embedding 3 Large + GPT 4 Turbo Ragas Score: 0.8819703122945702\n"
     ]
    }
   ],
   "source": [
    "large_4t_correctness, large_4t_ragas, large_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Correctness Score:\", large_4t_correctness)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Ragas Score:\", large_4t_ragas)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Ragas Score:\", large_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72d49841-eeeb-4115-83ac-5f5587328208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 100/100 [00:00<00:00, 1503.46file/s]\n",
      "100%|██████████| 100/100 [04:20<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:11<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:30<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:36<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 Large (256) + GPT 4 Turbo Small Correctness Score: 4.415\n",
      "Text Embedding 3 Large (256) + GPT 4 Turbo Small Ragas Score: {'context_precision': 0.7850, 'faithfulness': 0.9100, 'answer_relevancy': 0.9130, 'context_recall': 0.9450}\n",
      "Text Embedding 3 Large (256) + GPT 4 Turbo Ragas Score: 0.883719466224787\n"
     ]
    }
   ],
   "source": [
    "large_256_4t_correctness, large_256_4t_ragas, large_256_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", dimensions=256, llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Correctness Score:\", large_256_4t_correctness)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Ragas Score:\", large_256_4t_ragas)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Ragas Score:\", large_256_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "565a43d0-1064-4b19-94e3-0b0d821a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd9ceefc-f3d6-424a-a5a4-e995c4208be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Model</th>\n",
       "      <th>LLM Model</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>Ragas Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>4.410</td>\n",
       "      <td>0.880393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>4.435</td>\n",
       "      <td>0.878140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>4.425</td>\n",
       "      <td>0.879644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.879228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-4-turbo-preview</td>\n",
       "      <td>4.405</td>\n",
       "      <td>0.885669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>gpt-4-turbo-preview</td>\n",
       "      <td>4.445</td>\n",
       "      <td>0.881970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Embedding Model            LLM Model  Correctness  Ragas Score\n",
       "0  text-embedding-ada-002        gpt-3.5-turbo        4.410     0.880393\n",
       "1  text-embedding-3-small        gpt-3.5-turbo        4.435     0.878140\n",
       "2  text-embedding-3-large        gpt-3.5-turbo        4.425     0.879644\n",
       "3  text-embedding-3-small                gpt-4        4.275     0.879228\n",
       "4  text-embedding-3-small  gpt-4-turbo-preview        4.405     0.885669\n",
       "5  text-embedding-3-large  gpt-4-turbo-preview        4.445     0.881970"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = {\n",
    "    \"Embedding Model\": [\n",
    "        \"text-embedding-ada-002\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "    ],\n",
    "    \"LLM Model\": [\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "    ],\n",
    "    \"Correctness\": [\n",
    "        baseline_correctness,\n",
    "        small_35t_correctness,\n",
    "        large_35t_correctness,\n",
    "        small_4_correctness,\n",
    "        small_4t_correctness,\n",
    "        large_4t_correctness,\n",
    "    ],\n",
    "    \"Ragas Score\": [baseline_score, small_35t_score, large_35t_score, small_4_score, small_4t_score, large_4t_score],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(comparison)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e0246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
