{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf205f5-2483-4b65-b655-ad06f1253d7b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "This notebook provides an introduction to Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50c39d-3c47-4f34-88c2-84bbc54d020f",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use LlamaIndex, which provides some abstractions over underlying APIs used for building LLM applications. For more about RAG and LlamaIndex's definitions, see [here](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423cc82b-5f5c-4757-98b4-914a3ccad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder:\n",
      "San_Jose,_California.txt\n",
      "Oakland,_California.txt\n",
      "San_Francisco.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# We'll load documents that we've already downloaded in the Synthetic Dataset for RAG\n",
    "data_dir = os.path.join(os.path.abspath(\"\"), \".content\")\n",
    "input_files = glob(os.path.join(data_dir, \"*.txt\"))\n",
    "print(\"Files in folder:\")\n",
    "for f in input_files:\n",
    "    print(os.path.basename(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927087f9-9b52-4a92-8fea-5422b1172922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f115d5-00b0-4f8c-85e6-4d4654d3ac43",
   "metadata": {},
   "source": [
    "## A simple RAG with LlamaIndex (using defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64847c2d-83e9-4ac6-bac5-102d6864ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2089-5a78-42e7-aa87-5dd9b860d260",
   "metadata": {},
   "source": [
    "First, build the index from all the documents we have. Using default chunking, and embedding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01567917-d39e-4185-bc97-e2027df1c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1061.13file/s]\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4afe279f-b069-4135-8dd0-058d09f75e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2432d76-a5a9-4637-b682-871847d28c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first settlers in San Francisco were the Yelamu group of the Ramaytush people.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who were the first settlers in San Francisco?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "710cb97e-c7ad-4e31-aa42-097b956da4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Oakland has a Chinatown.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Does Oakland have a Chinatwon?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0340c103-b046-4ef5-b605-28795829300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Where does San Jose get its name from?\n",
      "Answer:\n",
      "San Jose gets its name from el Pueblo de San José de Guadalupe, which means \"the Town of Saint Joseph of Guadalupe\" in Spanish.\n",
      "Chunks:\n",
      "--------------------------\n",
      "== Name ==\n",
      "San Jose is named after el Pueblo de San José de Guadalupe (Spanish for 'the Town of Saint Joseph of Guadalupe'), the city's predecessor, which was eventually located in the area of what is now the Plaza de César Chávez. In the 19th century, print publications used the spelling \"San José\" for both the city and its eponymous township. On December 11, 1943, the United States Board on Geographic Names ruled that the city's name should be spelled \"San Jose\" based on local usage and the formal incorporated name.In the 1960s and 1970s, some residents and officials advocated for returning to the original spelling of \"San José\", with the acute accent on the \"e\", to acknowledge the city's Mexican origin and Mexican-American population. On June 2, 1969, the city adopted a flag designed by historian Clyde Arbuckle that prominently featured the inscription \"SAN JOSÉ, CALIFORNIA\". On June 16, 1970, San Jose State College officially adopted \"San José\" as the city's name, including in the college's own name. On August 20, 1974, the San Jose City Council approved a proposal by Catherine Linquist to rename the city \"San José\" but reversed itself a week later under pressure from residents concerned with the cost of changing typewriters, documents, and signs. On April 3, 1979, the city council once again adopted \"San José\" as the spelling of the city name on the city seal, official stationery, office titles and department names. As late as 2010, the 1965 city charter stated the name of the municipal corporation as City of San Jose, without the accent mark, but later editions have added the accent mark.By convention, the spelling San José is only used when the name is spelled in mixed upper- and lowercase letters, but not when the name is spelled only in uppercase letters, as on the city logo. The accent reflects the Spanish version of the name, and the dropping of accents in all-capital writing was once typical in Spanish. While San José is commonly spelled both with and without the acute accent over the \"e\", the city's official guidelines indicate that it should be spelled with the accent most of the time and sets forth narrow exceptions, such as when the spelling is in URLs, when the name appears in all-capital letters, when the name is used on social media sites where the diacritical mark does not render properly, and where San Jose is part of the proper name of another organization or business, such as San Jose Chamber of Commerce, that has chosen not to use the accent-marked name.\n",
      "\n",
      "\n",
      "== History ==\n",
      "\n",
      "\n",
      "=== Pre-colonial period ===\n",
      "San Jose, along with most of the Santa Clara Valley, has been home to the Tamien group (also spelled as Tamyen, Thamien) of the Ohlone people since around 4,000 BC. The Tamien spoke Tamyen language of the Ohlone language family.\n",
      "During the era of Spanish colonization and the subsequent building of Spanish missions in California, the Tamien people's lives changed dramatically. From 1777 onward, most of the Tamien people were forcibly enslaved at Mission Santa Clara de Asís or Mission San José where they were baptized and educated to be Catholic neophytes, also known as Mission Indians. This continued until the mission was secularized by the Mexican Government in 1833. A large majority of the Tamien died either from disease in the missions, or as a result of the state sponsored genocide. Some surviving families remained intact, migrating to Santa Cruz after their ancestral lands were granted to Spanish and Mexican Immigrants.\n",
      "--------------------------\n",
      "--------------------------\n",
      "=== Topography ===\n",
      "The Guadalupe River runs from the Santa Cruz Mountains flowing north through San Jose, ending in the San Francisco Bay at Alviso. Along the southern part of the river is the neighborhood of Almaden Valley, originally named for the mercury mines which produced mercury needed for gold extraction from quartz during the California Gold Rush as well as mercury fulminate blasting caps and detonators for the U.S. military from 1870 to 1945. East of the Guadalupe River, Coyote Creek also flows to south San Francisco Bay and originates on Mount Sizer near Henry W. Coe State Park and the surrounding hills in the Diablo Range, northeast of Morgan Hill, California.\n",
      "The lowest point in San Jose is 13 ft (4.0 m) below sea level at the San Francisco Bay in Alviso; the highest is 2,125 ft (648 m). Because of the proximity to Lick Observatory atop Mount Hamilton, San Jose has taken several steps to reduce light pollution, including replacing all street lamps and outdoor lighting in private developments with low pressure sodium lamps. To recognize the city's efforts, the asteroid 6216 San Jose was named after the city.There are four distinct valleys in the city of San Jose: Almaden Valley, situated on the southwest fringe of the city; Evergreen Valley to the southeast, which is hilly all throughout its interior; Santa Clara Valley, which includes the flat, main urban expanse of the South Bay; and the rural Coyote Valley, to the city's extreme southern fringe.The extensive droughts in California, coupled with the drainage of the reservoir at Anderson Lake for seismic repairs, have strained the city's water security. San Jose has suffered from lack of precipitation and water scarcity to the extent that some residents may run out of household water by the summer of 2022.\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"Where does San Jose get its name from?\"\n",
    "response = query_engine.query(question)\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"Answer:\")\n",
    "print(str(response))\n",
    "print(\"Chunks:\")\n",
    "for node in response.source_nodes:\n",
    "    print(\"--------------------------\")\n",
    "    print(str(node.text))\n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4e23b-0778-45b0-a38b-b91b4e59bc9b",
   "metadata": {},
   "source": [
    "## Evaluation of RAGs using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f031c8c-62c0-4a59-a724-16ced4263b17",
   "metadata": {},
   "source": [
    "### Load the Synthetic Dataset for Evaluating the RAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3294918-a5a5-40b3-81d5-3f1bcb39a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 47 rows\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.load_from_disk(\"rag_synth_data.hf\")\n",
    "print(f\"Dataset contains {len(dataset)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac14923-6efc-4071-9ab3-5bd80c0cab35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'reference_contexts', 'reference_answer', 'reference_answer_by', 'query_by'],\n",
       "    num_rows: 47\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fd6f-75df-42dd-9479-05d9f3812b04",
   "metadata": {},
   "source": [
    "## Ragas \n",
    "You need the following columns for Ragas Evaluation\n",
    "- question: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "- answer: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "- contexts: list[list[str]] - The contexts that were passed into the LLM to answer the question.\n",
    "- ground_truths: list[list[str]] - The ground truth answer to the questions. (only required if you are using context_recall)\n",
    "\n",
    "\n",
    "## Ragas Metrics:\n",
    "The harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.\n",
    "- faithfulness - the factual consistency of the answer to the context base on the question.\n",
    "- context_precision - a measure of how relevant the retrieved context is to the question. Conveys quality of the retrieval pipeline.\n",
    "- answer_relevancy - a measure of how relevant the answer is to the question\n",
    "- context_recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc4d7273-775d-4345-aa4b-25fdb080565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91c9a585-4337-47c9-923e-491a9949f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import harmonic_mean, mean\n",
    "\n",
    "import nest_asyncio\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from ragas import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def run_eval(embed_model=None, llm_model=None, dimensions=None):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    correctness_scores = []\n",
    "\n",
    "    if embed_model:\n",
    "        if dimensions:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model, dimensions=dimensions)\n",
    "        else:\n",
    "            embedding_model = OpenAIEmbedding(model=embed_model)\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=OpenAI())\n",
    "    else:\n",
    "        if llm_model:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI(model=llm_model))\n",
    "        else:\n",
    "            the_service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_files=input_files).load_data(\"*.txt\")\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=the_service_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm=OpenAI())\n",
    "    evaluator = CorrectnessEvaluator(service_context=service_context)\n",
    "\n",
    "    for row in tqdm(dataset):\n",
    "        # The Question\n",
    "        question = row[\"query\"]\n",
    "        questions.append(question)\n",
    "\n",
    "        # The Answer\n",
    "        response = query_engine.query(question)\n",
    "        answer = str(response)\n",
    "        answers.append(answer)\n",
    "\n",
    "        # Contexts\n",
    "        context = []\n",
    "        for node in response.source_nodes:\n",
    "            context.append(str(node.text))\n",
    "        contexts.append(context)\n",
    "\n",
    "        # Ground Truth\n",
    "        actual_answer = row[\"reference_answer\"]\n",
    "        ground_truths.append([actual_answer])\n",
    "\n",
    "        # Correctness with llama-index\n",
    "        correctness = evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=answer,\n",
    "            reference=actual_answer,\n",
    "        )\n",
    "        correctness_scores.append(correctness.score)\n",
    "\n",
    "    eval_dataset = Dataset.from_dict(\n",
    "        {\"question\": questions, \"contexts\": contexts, \"answer\": answers, \"ground_truths\": ground_truths}\n",
    "    )\n",
    "\n",
    "    result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "    baseline_ragas = result\n",
    "    ragas_score = harmonic_mean(list(result.values()))\n",
    "\n",
    "    return mean(correctness_scores), baseline_ragas, ragas_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc859626-c0d7-410d-9379-e0fa75e94580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 925.08file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [05:00<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.24s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:19<00:00, 19.97s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:17<00:00,  4.42s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:56<00:00, 14.03s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Correctness Score: 3.5\n",
      "Baseline Ragas Scores: {'context_precision': 0.1702, 'faithfulness': 0.5189, 'answer_relevancy': 0.7469, 'context_recall': 0.8431}\n",
      "Baseline Overall Ragas Scores: 0.38732844991231846\n"
     ]
    }
   ],
   "source": [
    "baseline_correctness, baseline_ragas, baseline_score = run_eval(\n",
    "    embed_model=\"text-embedding-ada-002\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Baseline Correctness Score:\", baseline_correctness)\n",
    "print(\"Baseline Ragas Scores:\", baseline_ragas)\n",
    "print(\"Baseline Overall Ragas Scores:\", baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bbbd55b-1094-4012-b8b5-b78f639660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 846.48file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [04:56<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:10<00:00, 17.64s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  4.63s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:07<00:00, 16.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 small + GPT 3.5 Turbo Small Correctness Score: 3.627659574468085\n",
      "Text Embedding 3 small + GPT 3.5 Turbo Small Ragas Score: {'context_precision': 0.2021, 'faithfulness': 0.5059, 'answer_relevancy': 0.7927, 'context_recall': 0.8095}\n",
      "Text Embedding 3 small + GPT 3.5 Turbo Overall Ragas Score: 0.4245976398525742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "small_35t_correctness, small_35t_ragas, small_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Correctness Score:\", small_35t_correctness)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Small Ragas Score:\", small_35t_ragas)\n",
    "print(\"Text Embedding 3 small + GPT 3.5 Turbo Overall Ragas Score:\", small_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba41066a-9215-4498-a42c-4927ae457936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1194.62file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [05:04<00:00,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.09s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:15<00:00, 18.87s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.82s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:02<00:00, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 Large Correctness Score: 3.648936170212766\n",
      "Text Embedding 3 Large Ragas Score: {'context_precision': 0.1702, 'faithfulness': 0.5038, 'answer_relevancy': 0.8044, 'context_recall': 0.7630}\n",
      "Text Embedding 3 Large Overall Ragas Score: 0.3841049958861384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "large_35t_correctness, large_35t_ragas, large_35t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large Correctness Score:\", large_35t_correctness)\n",
    "print(\"Text Embedding 3 Large Ragas Score:\", large_35t_ragas)\n",
    "print(\"Text Embedding 3 Large Overall Ragas Score:\", large_35t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25641cd4-dae7-470e-9124-d9cdff07f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 955.71file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [08:53<00:00, 11.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:14<00:00,  3.61s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:53<00:00, 13.35s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.14s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:09<00:00, 17.49s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 + GPT 4 Correctness Score: 2.6382978723404253\n",
      "Text Embedding 3 + GPT 4 Ragas Score: {'context_precision': 0.2021, 'faithfulness': 0.5534, 'answer_relevancy': 0.5839, 'context_recall': 0.8130}\n",
      "Text Embedding 3 + GPT 4 Overall Ragas Score: 0.412499768638822\n"
     ]
    }
   ],
   "source": [
    "small_4_correctness, small_4_ragas, small_4_score = run_eval(embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4\")\n",
    "print(\"Text Embedding 3 + GPT 4 Correctness Score:\", small_4_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Ragas Score:\", small_4_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Overall Ragas Score:\", small_4_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04fa1321-4da1-499c-afb8-61db1aa9f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1225.57file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [13:46<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                   | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.11/site-packages/pydantic/_internal/_core_utils.py:207: RuntimeWarning: coroutine 'CorrectnessEvaluator.aevaluate' was never awaited\n",
      "  return f(schema, self._walk)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:11<00:00,  2.94s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:05<00:00, 16.31s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.84s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:10<00:00, 17.68s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 + GPT 4 Turbo Small Correctness Score: 3.723404255319149\n",
      "Text Embedding 3 + GPT 4 Turbo Small Ragas Score: {'context_precision': 0.2021, 'faithfulness': 0.5595, 'answer_relevancy': 0.7303, 'context_recall': 0.7934}\n",
      "Text Embedding 3 + GPT 4 Turbo Ragas Score: 0.42714651703774953\n"
     ]
    }
   ],
   "source": [
    "small_4t_correctness, small_4t_ragas, small_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Correctness Score:\", small_4t_correctness)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Small Ragas Score:\", small_4t_ragas)\n",
    "print(\"Text Embedding 3 + GPT 4 Turbo Ragas Score:\", small_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "671ac9e5-b278-4b0d-ba18-2b08e1727475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 780.82file/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [15:57<00:00, 20.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:15<00:00,  3.88s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:04<00:00, 16.06s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  4.72s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:05<00:00, 16.48s/it]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding 3 Large + GPT 4 Turbo Small Correctness Score: 3.6382978723404253\n",
      "Text Embedding 3 Large + GPT 4 Turbo Small Ragas Score: {'context_precision': 0.1702, 'faithfulness': 0.5755, 'answer_relevancy': 0.7250, 'context_recall': 0.7523}\n",
      "Text Embedding 3 Large + GPT 4 Turbo Ragas Score: 0.3875577131648241\n"
     ]
    }
   ],
   "source": [
    "large_4t_correctness, large_4t_ragas, large_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-large\", llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Correctness Score:\", large_4t_correctness)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Small Ragas Score:\", large_4t_ragas)\n",
    "print(\"Text Embedding 3 Large + GPT 4 Turbo Ragas Score:\", large_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d49841-eeeb-4115-83ac-5f5587328208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 730.12file/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Embeddings.create() got an unexpected keyword argument 'dimensions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m large_256_4t_correctness, large_256_4t_ragas, large_256_4t_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-turbo-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText Embedding 3 Large (256) + GPT 4 Turbo Small Correctness Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, large_256_4t_correctness)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText Embedding 3 Large (256) + GPT 4 Turbo Small Ragas Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, large_256_4t_ragas)\n",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(embed_model, llm_model, dimensions)\u001b[0m\n\u001b[1;32m     29\u001b[0m         the_service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(llm\u001b[38;5;241m=\u001b[39mOpenAI())\n\u001b[1;32m     31\u001b[0m documents \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_files\u001b[38;5;241m=\u001b[39minput_files)\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthe_service_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n\u001b[1;32m     35\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(llm\u001b[38;5;241m=\u001b[39mOpenAI())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/base.py:112\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    105\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    106\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     service_context\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[1;32m    108\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:53\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/base.py:75\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 75\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:274\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    267\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    268\u001b[0m ):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:246\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:199\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 199\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:107\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m     99\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/indices/utils.py:137\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 137\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    142\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/core/embeddings/base.py:256\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cur_batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch_size:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# flush\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    253\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    254\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    255\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 256\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m         result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    258\u001b[0m         event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    259\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    260\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    261\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    262\u001b[0m             },\n\u001b[1;32m    263\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/embeddings/openai.py:413\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/llama_index/embeddings/openai.py:178\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[0;32m--> 178\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mTypeError\u001b[0m: Embeddings.create() got an unexpected keyword argument 'dimensions'"
     ]
    }
   ],
   "source": [
    "large_256_4t_correctness, large_256_4t_ragas, large_256_4t_score = run_eval(\n",
    "    embed_model=\"text-embedding-3-small\", dimensions=256, llm_model=\"gpt-4-turbo-preview\"\n",
    ")\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Correctness Score:\", large_256_4t_correctness)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Small Ragas Score:\", large_256_4t_ragas)\n",
    "print(\"Text Embedding 3 Large (256) + GPT 4 Turbo Ragas Score:\", large_256_4t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "565a43d0-1064-4b19-94e3-0b0d821a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd9ceefc-f3d6-424a-a5a4-e995c4208be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Model</th>\n",
       "      <th>LLM Model</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>Ragas Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.387328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.627660</td>\n",
       "      <td>0.424598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.648936</td>\n",
       "      <td>0.384105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>2.638298</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>gpt-4-turbo-preview</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>0.427147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>gpt-4-turbo-preview</td>\n",
       "      <td>3.638298</td>\n",
       "      <td>0.387558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Embedding Model            LLM Model  Correctness  Ragas Score\n",
       "0  text-embedding-ada-002        gpt-3.5-turbo     3.500000     0.387328\n",
       "1  text-embedding-3-small        gpt-3.5-turbo     3.627660     0.424598\n",
       "2  text-embedding-3-large        gpt-3.5-turbo     3.648936     0.384105\n",
       "3  text-embedding-3-small                gpt-4     2.638298     0.412500\n",
       "4  text-embedding-3-small  gpt-4-turbo-preview     3.723404     0.427147\n",
       "5  text-embedding-3-large  gpt-4-turbo-preview     3.638298     0.387558"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = {\n",
    "    \"Embedding Model\": [\n",
    "        \"text-embedding-ada-002\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-small\",\n",
    "        \"text-embedding-3-large\",\n",
    "    ],\n",
    "    \"LLM Model\": [\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "        \"gpt-4-turbo-preview\",\n",
    "    ],\n",
    "    \"Correctness\": [\n",
    "        baseline_correctness,\n",
    "        small_35t_correctness,\n",
    "        large_35t_correctness,\n",
    "        small_4_correctness,\n",
    "        small_4t_correctness,\n",
    "        large_4t_correctness,\n",
    "    ],\n",
    "    \"Ragas Score\": [baseline_score, small_35t_score, large_35t_score, small_4_score, small_4t_score, large_4t_score],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(comparison)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b2021-1890-41bc-8144-47bfc16f83da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
