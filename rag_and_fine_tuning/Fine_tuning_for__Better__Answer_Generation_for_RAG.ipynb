{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DJr1PofQn3H"
   },
   "source": [
    "# Fine-tuning for \"Better\" Answer Generation for RAG.\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ai-hero/workshop-keeping-up-with-openai-et-al/blob/main/rag_and_fine_tuning/Fine_tuning_for__Better__Answer_Generation_for_RAG.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Large Language Models are just that - language models. And should not be used as databases. But LLMs for RAG provide a unique opportunity. Since the context retrieved for RAG already contains the answer, the LLMs role is to frame it in the voice that the creator desires - e.g. specific terminology, format, structure, added guardrails, etc.\n",
    "\n",
    "\"Better\" here doesn't mean more accurate. It means that the LLM's output is framed (or phrased) more aptly for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFP_J2WHSClX"
   },
   "source": [
    "## First, let's install a few dependencies\n",
    "(`pip install -q <lib>` = quiet mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4vvEW61gQi7d"
   },
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv trl transformers peft accelerate bitsandbytes datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./my.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qi7_XzWQnBx"
   },
   "source": [
    "## Next, let's download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0nD-VxOCSwSW"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_split = load_dataset(\"rparundekar/rag_fine_tuning_500\", split=\"train\")\n",
    "val_split = load_dataset(\"rparundekar/rag_fine_tuning_500\", split=\"validation\")\n",
    "## NOTE: You'll need to set an env var HF_TOKEN (I've used colab secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWlFXbXwTgzw"
   },
   "source": [
    "The data (built from the `sciq` dataset) contains the user question (str), the contexts (list[str]), the answer our RAG returned, and the original answer in the dataset.\n",
    "\n",
    "The \"original\" answer is the answer we want our LLM to return. In practice, think of this as a corrected answer your human-in-the-loop annotators hacve prepared, or from user feedback. Here, it's the original short answer in sciq dataset - our goal is to fine tune the LLM to return short and correct answers from the context.\n",
    "\n",
    "The contexts and answers were generated assuming a simple RAG based retrieval (see the data generation notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "YpmTrszsTDkl",
    "outputId": "b50f0ce9-1dcd-4c65-c101-3ade690bd720"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-07830a61-95c0-4490-90eb-d1a9530a923c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>original_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>[Bacteria can be used to make cheese from milk...</td>\n",
       "      <td>Bacteria is commonly used in the preparation o...</td>\n",
       "      <td>Mesophilic Organisms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>[Without Coriolis Effect the global winds woul...</td>\n",
       "      <td>The phenomenon that makes global winds blow no...</td>\n",
       "      <td>Coriolis Effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>[Summary Changes of state are examples of phas...</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Exothermic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>[All radioactive decay is dangerous to living ...</td>\n",
       "      <td>Alpha decay is the least dangerous radioactive...</td>\n",
       "      <td>Alpha Decay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the worldâ€™s most continuo...</td>\n",
       "      <td>[Example 3.5 Calculating Projectile Motion: Ho...</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Smoke And Ash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07830a61-95c0-4490-90eb-d1a9530a923c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-07830a61-95c0-4490-90eb-d1a9530a923c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-07830a61-95c0-4490-90eb-d1a9530a923c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9d7c4a47-0384-456b-a1ff-f523eeceba4f\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d7c4a47-0384-456b-a1ff-f523eeceba4f')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9d7c4a47-0384-456b-a1ff-f523eeceba4f button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What type of organism is commonly used in prep...   \n",
       "1  What phenomenon makes global winds blow northe...   \n",
       "2  Changes from a less-ordered state to a more-or...   \n",
       "3     What is the least dangerous radioactive decay?   \n",
       "4  Kilauea in hawaii is the worldâ€™s most continuo...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Bacteria can be used to make cheese from milk...   \n",
       "1  [Without Coriolis Effect the global winds woul...   \n",
       "2  [Summary Changes of state are examples of phas...   \n",
       "3  [All radioactive decay is dangerous to living ...   \n",
       "4  [Example 3.5 Calculating Projectile Motion: Ho...   \n",
       "\n",
       "                                              answer       original_answer  \n",
       "0  Bacteria is commonly used in the preparation o...  Mesophilic Organisms  \n",
       "1  The phenomenon that makes global winds blow no...       Coriolis Effect  \n",
       "2                                         exothermic            Exothermic  \n",
       "3  Alpha decay is the least dangerous radioactive...           Alpha Decay  \n",
       "4                                      smoke and ash         Smoke And Ash  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFITxD2rUczE"
   },
   "source": [
    "The dataset is also really small - only 500 rows. Depending on your use case, we typically would need to train on 1000+ rows for a good base model (e.g. OpenAI) or more for most open source models.\n",
    "\n",
    "Here, the answer is already in the context. So we're not fine tuning the model to remember the sciq dataset - just rephrase the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbTGNA0WToDL",
    "outputId": "98731879-7cfe-420a-fd98-5dd382364b6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFxvLVIHVKvk"
   },
   "source": [
    "Our fine tuning task is to provide succinct answers. In this example, you can see that the original answer is more descriptive.\n",
    "\n",
    "For your use case, it could be a different format, you could use fewer technical terms, etc. LLM fine tuning is all about changing that phrasing/framing of the response. DON'T THINK IT'S LIKE A DATABASE AND ADD MORE DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mVR2m9rUapE",
    "outputId": "67cb9d8c-f009-4211-875d-f64472417716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "Answer: Bacteria is commonly used in the preparation of foods such as cheese and yogurt.\n",
      "Updated Answer: Mesophilic Organisms\n"
     ]
    }
   ],
   "source": [
    "example = train_split[0]\n",
    "question = example[\"question\"]\n",
    "answer = example[\"answer\"]\n",
    "target = example[\"original_answer\"]\n",
    "print(\n",
    "    f\"\"\"Example:\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "Updated Answer: {target}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsSMNef4V0w7"
   },
   "source": [
    "## Let's set up our training data and model\n",
    "We'll load the model, create a generator function to format the data.\n",
    "\n",
    "Because Meta requires you to agree to terms before you can use Llama 2, you'll need to apply for access on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c961ea592fbb444097b15dd7a6d9b4c3",
      "a5d085917f1c44eb84eb1b82f9b3a613",
      "28b598fa52d64d4b9e2cd312facd052e",
      "ffdabfbac84b4b3191ea09b4a68bb889",
      "47faf2617dfe45e1a1e255ddc09c7f7a",
      "67684a50bb1445bdad3fd5eee739662f",
      "8c2f39d329e8484ba430327367f87bd8",
      "f27ddd082b714af9a9cd51b6747d19e2",
      "65ea0572b99f4822995274b24340a3a4",
      "fcf9e8cef4184255b41c9c6de94aeec2",
      "970e88bb7e3048c78658af849e70588a"
     ]
    },
    "id": "h1SaPbrnUyqb",
    "outputId": "e53f9f17-3099-4fc8-b4ee-a48b9aeefe16"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c961ea592fbb444097b15dd7a6d9b4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's load the model.\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# The base model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Quantization to fit on T4 GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZES7s9WFYnCr"
   },
   "source": [
    "Next, lets's set up our tokenizer and a function to build the input in an instruction format and tokenize. We'll add our BOS and EOS (beginning and end tokens) in our code, instead of auto adding them. This gives us more control during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tz4t6zG0WiGj",
    "outputId": "ee5a8647-52bb-41f1-eb90-e3e69506eb1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=False, add_bos_token=False, trust_remote_code=True)\n",
    "special_tokens = {\"pad_token\": \"[PAD]\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# We need to resize token embeddings length in the model.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFrXzWN-dq9I"
   },
   "source": [
    "Update the dataset to add a \"text\" field with the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HTYS8ltNZDBE"
   },
   "outputs": [],
   "source": [
    "def generate_text(row):\n",
    "    question = row[\"question\"]\n",
    "    contexts = \"\\n\".join(row[\"contexts\"])\n",
    "\n",
    "    prompt = f\"\"\"<s>### Question:\n",
    "{question}\n",
    "\n",
    "### Contexts:\n",
    "{contexts}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    if \"original_answer\" in row:\n",
    "        answer = row[\"original_answer\"]\n",
    "        prompt += f\"{answer}</s>\"\n",
    "    row[\"text\"] = prompt\n",
    "    return row\n",
    "\n",
    "\n",
    "train_split_ds = train_split.map(generate_text)\n",
    "val_split_ds = train_split.map(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9kCSAqKg5mj",
    "outputId": "bf40c140-be5a-406d-d349-19895e7446d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Question:\n",
      "What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "\n",
      "### Contexts:\n",
      "Bacteria can be used to make cheese from milk. The bacteria turn the milk sugars into lactic acid. The acid is what causes the milk to curdle to form cheese. Bacteria are also involved in producing other foods. Yogurt is made by using bacteria to ferment milk ( Figure below ). Fermenting cabbage with bacteria produces sauerkraut.\n",
      "Humans have collected and grown mushrooms for food for thousands of years. Figure below shows some of the many types of mushrooms that people eat. Yeasts are used in bread baking and brewing alcoholic beverages. Other fungi are used in fermenting a wide variety of foods, including soy sauce, tempeh, and cheeses. Blue cheese has its distinctive appearance and flavor because of the fungus growing though it (see Figure below ).\n",
      "\n",
      "### Answer:\n",
      "Mesophilic Organisms</s>\n"
     ]
    }
   ],
   "source": [
    "print(train_split_ds[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBMgM16Myv8Z"
   },
   "source": [
    "**NOTE THAT IN THE ABOVE PROMPT, WE DO NOT HAVE ANY INSTRUCTIONS FOR THE MODEL**\n",
    "\n",
    "Our hypothesis is that when we're going to fine tune, the model will also learn instructions. And so instead of building a general model we are making our model more specific to our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmAB6oqEdVg3"
   },
   "source": [
    "Let's set up the model for PEFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hG5CL63ca103"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # These layers vary by different models\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaTd31D4gqy5"
   },
   "source": [
    "Create the trainer (we're using SFTTrainer from Huggingface's TRL library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yGPiuqwmaAnK"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=250,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=500,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_split_ds,\n",
    "    eval_dataset=val_split_ds,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZt97rM2yRLt"
   },
   "source": [
    "Let's see how our base model performs without any fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35nZZeMvyV-V",
    "outputId": "cafa5d84-08d3-4b36-9638-4f987033b584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How far is the moon from the earth?\n",
      "Answer: The moon is 238,900 miles from the Earth.\n",
      "\n",
      "### Solution:\n",
      "\n",
      "```python\n",
      "from math import *\n",
      "\n",
      "earth_radius = 3959\n",
      "moon_radius = 1737.1\n",
      "moon_distance = 238900\n",
      "\n",
      "earth_radius = 3959\n",
      "moon_radius = 1737.1\n",
      "moon_distance = 238900\n",
      "```\n",
      "\n",
      "### Source:\n",
      "[Wikipedia](https://en.wikipedia.org/wiki/Moon)\n",
      "\n",
      "### Notes:\n",
      "\n",
      "### Hints:\n",
      "\n",
      "### Attributions:\n",
      "\n",
      "Question: How far is the moon from the earth?\n",
      "Answer: ```\n",
      "384,400 km\n",
      "```\n",
      "\n",
      "### Source:\n",
      "[https://www.britannica.com/science/moon/Moon-Physics-and-Mechanics](https://www.britannica.com/science/moon/Moon-Physics-and-Mechanics)\n",
      "\n",
      "### Links:\n",
      "[https://en.wikipedia.org/wiki/Moon](https://en.wikipedia.org/wiki/Moon)\n",
      "\n",
      "Question: How far is the moon from the earth?\n",
      "Answer: The Moon is 238,900 miles (384,400 km) from Earth.\n",
      "\n",
      "### Source:\n",
      "[https://en.wikipedia.org/wiki/Moon#Orbit](https://en.wikipedia.org/wiki/Moon#Orbit)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "def generate(prompt, tokenizer, model):\n",
    "    \"\"\"Generate a completion from a prompt.\"\"\"\n",
    "    gen_config = GenerationConfig.from_pretrained(model.name_or_path, max_new_tokens=512)\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", padding=True)[\"input_ids\"].cuda()\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(inputs=tokenized_prompt, generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]) :], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "row = {\n",
    "    \"question\": \"How far is the moon from the earth?\",\n",
    "    \"contexts\": [\n",
    "        \"The Moon is Earth's only natural satellite. It orbits at an average distance of 384,400 km (238,900 mi), \\\n",
    "about 30 times Earth's diameter. The Moon always presents the same side to Earth, because gravitational pull has \\\n",
    "locked its rotation to the planet. This results in the lunar day of 29.5 Earth days matching the lunar month. \\\n",
    "The Moon's gravitational pull â€“ and to a lesser extent the Sun's â€“ are the main drivers of the tides.\"\n",
    "    ],\n",
    "}\n",
    "prompt = generate_text(row)[\"text\"].strip()\n",
    "\n",
    "for i in range(3):\n",
    "    completion = generate(prompt, tokenizer, model)\n",
    "    print(f\"Question: {row['question']}\\nAnswer: {completion}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Ke5_VeyXqh"
   },
   "source": [
    "As you can see, the answer is there, but it's not as short as we'd like. It's also not as consistent. For example, we want the output to just say **\"384,400 km (238,900 mi)\"**\n",
    "\n",
    "If your dataset doesn't talk about public datadata, chances are that it's going to not even know this answer.\n",
    "\n",
    "\n",
    "Now, let's fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "id": "4GCxh5jvggw1",
    "outputId": "671a0b5d-1e52-469c-e280-227d256d81a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 30:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.443600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.326797664642334, metrics={'train_runtime': 1861.6419, 'train_samples_per_second': 0.269, 'train_steps_per_second': 0.269, 'total_flos': 6171803433200832.0, 'train_loss': 1.326797664642334, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBN8qTA6lnn4"
   },
   "source": [
    "You can see that the loss value is sort of going down. It's learning!\n",
    "\n",
    "Let's be honest. This is not the way we do Data Science. We need to have a larger dataset, split into train, val and test splits. And then watch for the loss curves for overfitting and underfitting. Early stop when val loss goes up. Out of scope for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLJ5N0p2mlYo"
   },
   "source": [
    "## Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1_vikDFhukK",
    "outputId": "b828e3a8-c664-4d51-9cfe-fef40e153bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How far is the moon from the earth?\n",
      "Answer: 384,400 Km\n",
      "-----------------------------------------\n",
      "\n",
      "Question: How far is the moon from the earth?\n",
      "Answer: 384,400 Km\n",
      "-----------------------------------------\n",
      "\n",
      "Question: How far is the moon from the earth?\n",
      "Answer: 384,400 Km\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run text generation with our fine tuned model\n",
    "row = {\n",
    "    \"question\": \"How far is the moon from the earth?\",\n",
    "    \"contexts\": [\n",
    "        \"The Moon is Earth's only natural satellite. It orbits at an average distance of 384,400 km (238,900 mi), \\\n",
    "about 30 times Earth's diameter. The Moon always presents the same side to Earth, because gravitational pull has \\\n",
    "locked its rotation to the planet. This results in the lunar day of 29.5 Earth days matching the lunar month. \\\n",
    "The Moon's gravitational pull â€“ and to a lesser extent the Sun's â€“ are the main drivers of the tides.\"\n",
    "    ],\n",
    "}\n",
    "prompt = generate_text(row)[\"text\"].strip()\n",
    "for i in range(3):\n",
    "    completion = generate(prompt, tokenizer, model)\n",
    "    print(f\"Question: {row['question']}\\nAnswer: {completion}\\n-----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqfGrgy4z9xE"
   },
   "source": [
    "As you can see, the model is now consistent in its output and able to answer the question using the context.\n",
    "\n",
    "\n",
    "Again, evaluating the model with one off examples is a bad idea. We need a more robust way of doing this. For example, is our model now going to hallucinate more? We should add \"I don't know\" or red teamed examples in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwAb-FAG0DTh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28b598fa52d64d4b9e2cd312facd052e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f27ddd082b714af9a9cd51b6747d19e2",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65ea0572b99f4822995274b24340a3a4",
      "value": 2
     }
    },
    "47faf2617dfe45e1a1e255ddc09c7f7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65ea0572b99f4822995274b24340a3a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "67684a50bb1445bdad3fd5eee739662f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c2f39d329e8484ba430327367f87bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "970e88bb7e3048c78658af849e70588a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5d085917f1c44eb84eb1b82f9b3a613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67684a50bb1445bdad3fd5eee739662f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8c2f39d329e8484ba430327367f87bd8",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c961ea592fbb444097b15dd7a6d9b4c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5d085917f1c44eb84eb1b82f9b3a613",
       "IPY_MODEL_28b598fa52d64d4b9e2cd312facd052e",
       "IPY_MODEL_ffdabfbac84b4b3191ea09b4a68bb889"
      ],
      "layout": "IPY_MODEL_47faf2617dfe45e1a1e255ddc09c7f7a"
     }
    },
    "f27ddd082b714af9a9cd51b6747d19e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcf9e8cef4184255b41c9c6de94aeec2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffdabfbac84b4b3191ea09b4a68bb889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcf9e8cef4184255b41c9c6de94aeec2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_970e88bb7e3048c78658af849e70588a",
      "value": " 2/2 [01:13&lt;00:00, 33.53s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
